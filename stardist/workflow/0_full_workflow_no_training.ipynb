{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:08:50.040933700Z",
     "start_time": "2024-06-14T14:08:46.093601Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tifffile import imread, imwrite\n",
    "from functions import *\n",
    "import json\n",
    "import geojson\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe1e6f83c2e13d",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dfbbe71819c75ad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Input path to WSIs, only tested with ndpi and svs but it works fine for both\n",
    "Input path to the StarDist model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb455a5311df807c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:08:50.054933600Z",
     "start_time": "2024-06-14T14:08:50.040933700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WSI_path = r'\\\\puppyserverdw\\Andre_pup\\CODA Alzheimers brain\\nuc_segment'\n",
    "model_path = r'\\\\10.162.80.16\\Andre_expansion\\data\\Stardist\\PDAC model\\models\\lea_model'\n",
    "file_type = '.ndpi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82b1833b546b785f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:08:51.799443900Z",
     "start_time": "2024-06-14T14:08:50.055933500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base_model.py (149): output path for model already exists, files may be overwritten: \\\\10.162.80.16\\Andre_expansion\\data\\Stardist\\PDAC model\\models\\lea_model\\offshoot_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default values: prob_thresh=0.5, nms_thresh=0.4.\n",
      "Overriding defaults: Thresholds(prob=0.4872387821889486, nms=0.3) \n"
     ]
    }
   ],
   "source": [
    "WSIs = [os.path.join(WSI_path, f) for f in os.listdir(WSI_path) if f.endswith(file_type)]  # change if using tif or czi file\n",
    "#WSIs = [os.path.join(WSI_path, f) for f in os.listdir(WSI_path) if f.endswith('.svs')]  # change if using tif or czi file\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f1abcb88bdf26",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Change date if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6afd9551a53f7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:09:01.150113100Z",
     "start_time": "2024-06-14T14:09:01.022113400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default values: prob_thresh=0.5, nms_thresh=0.4.\n",
      "Overriding defaults: Thresholds(prob=0.4872387821889486, nms=0.3) \n",
      "\n",
      "\\\\puppyserverdw\\Andre_pup\\CODA Alzheimers brain\\nuc_segment\\StarDist_6_14_24_PDAC\\json\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_path)\n",
    "date = '6_14_24_PDAC'\n",
    "\n",
    "out_pth = os.path.join(WSI_path, f'StarDist_{date}')\n",
    "if not os.path.exists(out_pth):\n",
    "    os.mkdir(out_pth)\n",
    "\n",
    "out_pth_json = os.path.join(out_pth, 'json')\n",
    "out_pth_tif = os.path.join(out_pth, 'tif')\n",
    "print(out_pth_json)\n",
    "\n",
    "if not os.path.exists(out_pth_json):\n",
    "    os.mkdir(out_pth_json)\n",
    "\n",
    "if not os.path.exists(out_pth_json):\n",
    "    os.mkdir(out_pth_json)\n",
    "\n",
    "if not os.path.exists(out_pth_tif):\n",
    "    os.mkdir(out_pth_tif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14888e53a27cd8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Segmentation output is a .geojson file with centroid/contour coordiantes\n",
    "If you want to save a tif, uncomment the commented lines within the for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2ff82617fdea54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:14:56.385014Z",
     "start_time": "2024-06-14T14:09:04.992842Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting WT12FB2_0339.ndpi\n",
      "effective: block_size=(4096, 4096, 3), min_overlap=(128, 128, 0), context=(128, 128, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [03:53<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving json...\n",
      "Finished WT12FB2_0339.json\n",
      "Saving tif...\n"
     ]
    }
   ],
   "source": [
    "save_tif = False\n",
    "\n",
    "# Segment all WSIs -- takes about 2-5 minutes per whole slide image to segment for 0.5 GB file, about 3 minutes to save geojson file\n",
    "for img_pth in WSIs:\n",
    "    try:\n",
    "        name = os.path.basename(img_pth)\n",
    "\n",
    "        if not os.path.exists(os.path.join(out_pth_json, (name[:-5] + '.json'))):\n",
    "            print(f'Starting {name}')\n",
    "            \n",
    "            img = imread(img_pth)\n",
    "            img = img/255  # normalization used to train model\n",
    "            \n",
    "            if save_tif:\n",
    "                _, polys = model.predict_instances_big(img, axes='YXC', block_size=4096, min_overlap=128, context=128, n_tiles=(4,4,1))\n",
    "                # labels, polys = model.predict_instances_big(img, axes='YXC', block_size=4096, min_overlap=128, context=128, n_tiles=(4,4,1))\n",
    "    \n",
    "                print('Saving json...')\n",
    "                save_json_from_WSI_pred(polys, out_pth_json, name)\n",
    "\n",
    "            else:\n",
    "                # tif file is like 3 GB usually, so only uncomment next part if you are ok with that\n",
    "                labels, polys = model.predict_instances_big(img, axes='YXC', block_size=4096, min_overlap=128, context=128, n_tiles=(4,4,1))\n",
    "                print('Saving json...')\n",
    "                save_json_from_WSI_pred(polys, out_pth_json, name)\n",
    "                \n",
    "                print('Saving tif...')\n",
    "                imwrite(os.path.join(out_pth_tif, name[:-5] + '.tif'), labels)\n",
    "                \n",
    "        else:\n",
    "            print(f'Skipping {name}')\n",
    "    except:\n",
    "        print(f'skipping {img_pth}, probably bc its too big...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5242332a8dfa1dfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:16:18.670270300Z",
     "start_time": "2024-06-14T14:16:18.647439400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\\\\\puppyserverdw\\\\Andre_pup\\\\CODA Alzheimers brain\\\\nuc_segment\\\\StarDist_6_14_24_PDAC\\\\json\\\\WT12FB2_0339.json']\n"
     ]
    }
   ],
   "source": [
    "json_pth_list = sorted([os.path.join(out_pth_json,file) for file in os.listdir(out_pth_json) if file.endswith(\".json\")])\n",
    "print(json_pth_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ce3361070ae06",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "make geojson file to load over ndpi file in qupath for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfb0a263a657ef32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T14:17:35.654649700Z",
     "start_time": "2024-06-14T14:16:21.469312200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1\n",
      "WT12FB2_0339.json\n",
      "Finished \\\\puppyserverdw\\Andre_pup\\CODA Alzheimers brain\\nuc_segment\\StarDist_6_14_24_PDAC\\WT12FB2_0339.geojson\n"
     ]
    }
   ],
   "source": [
    "for p, file in enumerate([json_pth_list[0]]):\n",
    "    nm = file.split('\\\\')[-1]\n",
    "    new_fn = os.path.join(out_pth, nm[:-5] + '.geojson')\n",
    "    print(f'{p} / {len(json_pth_list)}')\n",
    "    print(nm)\n",
    "    \n",
    "    if not os.path.exists(new_fn):\n",
    "    \n",
    "        segmentation_data = json.load(open(file))\n",
    "        \n",
    "        downsize_amount = 1 # keep this unless you want to scale to a 10x image or something\n",
    "        data_list = format_seg_data(segmentation_data, downsize_amount)\n",
    "    \n",
    "        GEOdata = []\n",
    "        \n",
    "        for j, (centroid, contour) in enumerate(data_list):\n",
    "            \n",
    "            #if j == 100000:\n",
    "            #    break\n",
    "            \n",
    "            centroid = [centroid[0] + 0, centroid[1] + 0]\n",
    "            # xy coordinates are swapped, so I reverse them here with xy[::-1]\n",
    "            # note: add 1 to coords to fix 0 indexing vs 1 index offset\n",
    "            contour = [[coord+0 for coord in xy[::-1]] for xy in contour]  # Convert coordinates to integers\n",
    "            contour.append(contour[0]) # stardist doesn't close the circle, needed for qupath\n",
    "        \n",
    "            # Create a new dictionary for each contour\n",
    "            dict_data = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"id\": \"PathCellObject\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Polygon\",\n",
    "                    \"coordinates\": [contour]\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    'objectType': 'annotation',\n",
    "                    'classification': {'name': 'Nuclei', 'color': [97, 214, 59]}\n",
    "                }\n",
    "            }\n",
    "        \n",
    "            GEOdata.append(dict_data)\n",
    "        \n",
    "        with open(new_fn,'w') as outfile:\n",
    "            geojson.dump(GEOdata,outfile)\n",
    "        print('Finished',new_fn)\n",
    "    \n",
    "    else:\n",
    "        print(f'skipping {new_fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae36a3bee7e3eb70",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make pickle file with nuclear features output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d2a121eb5e21ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:39:30.076163300Z",
     "start_time": "2024-06-12T19:39:28.919593300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from analysis_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbb6c6a0a7628d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T19:39:30.152163Z",
     "start_time": "2024-06-12T19:39:30.077163300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_pth_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pkl_pth \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mout_pth_json\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnuclei_features_pkls\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pkl_pth): os\u001b[38;5;241m.\u001b[39mmkdir(pkl_pth)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(pkl_pth)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'out_pth_json' is not defined"
     ]
    }
   ],
   "source": [
    "pkl_pth = os.path.join(out_pth_json,'nuclei_features_pkls')\n",
    "if not os.path.exists(pkl_pth): os.mkdir(pkl_pth)\n",
    "print(pkl_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea38f9a59b451c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T17:25:51.213243700Z",
     "start_time": "2024-06-12T17:25:51.211243800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, json_f_name in enumerate(json_pth_list):\n",
    "    \n",
    "    nm = json_f_name.split('\\\\')[-1].split('.')[0]\n",
    "    \n",
    "    outnm = os.path.join(pkl_pth, f'{nm}.pkl')\n",
    "    print(outnm)\n",
    "    \n",
    "    if not os.path.exists(outnm):\n",
    "        \n",
    "        HE_20x_WSI = imread(WSIs[i])\n",
    "        \n",
    "        print(WSIs[i])\n",
    "        print(json_f_name)\n",
    "        \n",
    "        try:\n",
    "            segmentation_data = json.load(open(json_f_name))\n",
    "        except:\n",
    "            print(f'error reading json... Skipping {json_f_name}')\n",
    "            continue\n",
    "    \n",
    "        centroids = [nuc['centroid'][0] for nuc in segmentation_data]\n",
    "        contours = [nuc['contour'] for nuc in segmentation_data]\n",
    "        contours_fixed = fix_contours(contours)\n",
    "        \n",
    "        # part of code below gets rgb intensity values within each nucleus contour, to do this efficiently, it crops\n",
    "        # a small part of the image for each nucleus. The offset variable determines how big the crop is. 30 should \n",
    "        # be fine for 20x images with normal/large sized nuclei but you can play with it yourself if you want\n",
    "        # it will skip nuclei that are by the edge and thus can't crop that image, it calls these intensities -1\n",
    "        \n",
    "        offset = 30  # radius of image that gets cropped from WSI, used for getting rgb intensity average inside nuc contour\n",
    "        \n",
    "        centroids_np = np.array(centroids)  # for other formatting\n",
    "        contours_np = np.array(contours)\n",
    "        \n",
    "        r_avg_list = []\n",
    "        g_avg_list = []\n",
    "        b_avg_list = []\n",
    "        \n",
    "        areas = []\n",
    "        perimeters = []\n",
    "        circularities = []\n",
    "        aspect_ratios = []\n",
    "        image_ids = []\n",
    "        classes = []\n",
    "        \n",
    "        compactness_a, eccentricity_a, euler_number_a, extent_a, form_factor_a, maximum_radius_a, mean_radius_a, median_radius_a, minor_axis_length_a, major_axis_length_a, orientation_degrees_a = [], [], [], [], [], [], [], [], [], [], []\n",
    "        \n",
    "        np_centroids = np.array(centroids)\n",
    "        \n",
    "        for j in range(len(contours_fixed)):\n",
    "            #break\n",
    "            \n",
    "            centroid = centroids[j]\n",
    "            # print(f'centroid: {centroid}')\n",
    "            contour_raw = copy.copy(contours_fixed[j])  # used for rgb intensities\n",
    "             \n",
    "            # get rbg intensity averages\n",
    "            r_avg, g_avg, b_avg = get_rbg_avg(centroid, contour_raw, offset, HE_20x_WSI)\n",
    "            # print(r_avg, g_avg, b_avg)\n",
    "            \n",
    "            r_avg_list.append(r_avg)\n",
    "            g_avg_list.append(g_avg)\n",
    "            b_avg_list.append(b_avg)\n",
    "            \n",
    "            contour = contours_np[j][0].transpose()  # used for other stuff, too lazy to make formatting the same\n",
    "            area = cntarea(contour)\n",
    "            perimeter = cntperi(contour)\n",
    "            circularity = 4 * np.pi * area / perimeter ** 2\n",
    "            MA = cntMA(contour)\n",
    "            [MA, ma, orientation] = MA\n",
    "            aspect_ratio = MA / ma\n",
    "            #center_x = centroid[0]\n",
    "            #center_y = centroid[1]\n",
    "            \n",
    "            cent_x = np_centroids[j,0]\n",
    "            cent_y = np_centroids[j,1]\n",
    "            \n",
    "            #compactness and form_factor are stupid because they are basically same as circularity, maybe extent too\n",
    "            \n",
    "            compactness = perimeter ** 2 / area\n",
    "            eccentricity = np.sqrt(1 - (ma / MA) ** 2)\n",
    "            extent = area / (MA * ma)\n",
    "            form_factor = (perimeter ** 2) / (4 * np.pi * area)\n",
    "            major_axis_length = MA\n",
    "            maximum_radius = np.max(np.linalg.norm(contour - centroid, axis=1))\n",
    "            mean_radius = np.mean(np.linalg.norm(contour - centroid, axis=1))\n",
    "            median_radius = np.median(np.linalg.norm(contour - centroid, axis=1))\n",
    "            minor_axis_length = ma\n",
    "            orientation_degrees = np.degrees(orientation)\n",
    "            \n",
    "            areas.append(area)\n",
    "            perimeters.append(perimeter)\n",
    "            circularities.append(circularity)\n",
    "            aspect_ratios.append(aspect_ratio)\n",
    "    \n",
    "            # additional features\n",
    "            compactness_a.append(compactness)\n",
    "            eccentricity_a.append(eccentricity)\n",
    "            extent_a.append(extent)\n",
    "            form_factor_a.append(form_factor)\n",
    "            maximum_radius_a.append(maximum_radius)\n",
    "            mean_radius_a.append(mean_radius)\n",
    "            median_radius_a.append(median_radius)\n",
    "            minor_axis_length_a.append(minor_axis_length)\n",
    "            major_axis_length_a.append(major_axis_length)\n",
    "            orientation_degrees_a.append(orientation_degrees)\n",
    "            \n",
    "            \n",
    "        # exit loop\n",
    "            \n",
    "        dat = {\n",
    "            'Centroid_x': np_centroids[:,1],\n",
    "            'Centroid_y': np_centroids[:,0],\n",
    "            'Area': areas,\n",
    "            'Perimeter': perimeters,\n",
    "            'Circularity': circularities,\n",
    "            'Aspect Ratio': aspect_ratios,\n",
    "            'compactness' : compactness_a,\n",
    "            'eccentricity' : eccentricity_a,\n",
    "            'extent' : extent_a,\n",
    "            'form_factor' : form_factor_a,\n",
    "            'maximum_radius' : maximum_radius_a,\n",
    "            'mean_radius' : mean_radius_a,\n",
    "            'median_radius' : median_radius_a,\n",
    "            'minor_axis_length' : minor_axis_length_a,\n",
    "            'major_axis_length' : major_axis_length_a,\n",
    "            'orientation_degrees' : orientation_degrees_a,\n",
    "            'r_mean_intensity' : r_avg_list,\n",
    "            'g_mean_intensity' : g_avg_list,\n",
    "            'b_mean_intensity' : b_avg_list,\n",
    "            #'slide_num': nm[-4:]  # fix this for your own needs, this gets slide number for my monkey fetus\n",
    "        }\n",
    "    \n",
    "        df = pd.DataFrame(dat).astype(np.float32)  # save a little space with float16 type -> Edit 2 months later, this did not save time.\n",
    "        \n",
    "        df.to_pickle(outnm)\n",
    "        #break\n",
    "    else:\n",
    "        print('skipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b68cf9619a5cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T17:23:20.776865400Z",
     "start_time": "2024-06-12T17:23:20.772865600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_pth = os.path.join(pkl_pth,'nuclei_features_mats')\n",
    "if not os.path.exists(mat_pth): os.mkdir(mat_pth)\n",
    "print(mat_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca067470fb0b78",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-12T17:23:20.773865500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.io import savemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f765debd60206f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-12T17:23:20.773865500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I am pretty sure that this will work but I couldn't test it on the folder I was working with bc I don't have permission but just ask chatGPT if it doesn't work\n",
    "\n",
    "dfs = [os.path.join(pkl_pth,f) for f in os.listdir(pkl_pth)]\n",
    "\n",
    "for dfnm in dfs:\n",
    "    \n",
    "    outnm = os.path.join(mat_pth,os.path.basename(dfnm))\n",
    "    \n",
    "    print(\"Saving: {}\".format(dfnm))\n",
    "        \n",
    "    with open(os.path.join(dfnm), 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "\n",
    "    col_names = df.columns.tolist()\n",
    "    df = [_ for _ in df.to_numpy()]\n",
    "    df = np.array(df)\n",
    "    \n",
    "    savemat(outnm, {'features':df, 'feature_names':col_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd39cb3ea9369f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T17:05:07.386791600Z",
     "start_time": "2024-06-12T17:05:07.382791600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
